{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this project, we tackle the problem of Text classification. Unlike the classification problems that we've seen in this class, the data used in text classification is non-numeric. Hence, one of the challenges of project is to find ways to transform our text data into numeric data that can be trained and tested using classification models such as Logistical Regression. One such method of transforming our data is vectorizing our text data into a \"Bag of Word\". However, as we will see throughout this notebook, simply using a \"Bag of Word\" is not optimal but there are many Natural Language Processing(NLP) techniques that we can use to optimize our data and improve the performance of our classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For this project, we used Software reviews from Amazon. This dataset and other Amazon reviews can be found in the following link: http://deepyeti.ucsd.edu/jianmo/amazon/index.html?fbclid=IwAR1oMrSAwo2dc48WhzGEzMUdPrpv6Mv0S_zGvQzcgDoyNW3_IvxHCmwD7FY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>02 23, 2014</td>\n",
       "      <td>A1YCCU0YRLS0FE</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Rosalind White Ames</td>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>Health</td>\n",
       "      <td>1393113600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>02 17, 2014</td>\n",
       "      <td>A1BJHRQDYVAY2J</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Allan R. Baker</td>\n",
       "      <td>IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...</td>\n",
       "      <td>ARE YOU KIDING ME?</td>\n",
       "      <td>1392595200</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>02 17, 2014</td>\n",
       "      <td>APRDVZ6QBIQXT</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Lucy</td>\n",
       "      <td>This book was missing pages!!! Important pages...</td>\n",
       "      <td>missing pages!!</td>\n",
       "      <td>1392595200</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>10 14, 2013</td>\n",
       "      <td>A2JZTTBSLS1QXV</td>\n",
       "      <td>0077775473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Albert V.</td>\n",
       "      <td>I have used LearnSmart and can officially say ...</td>\n",
       "      <td>Best study product out there!</td>\n",
       "      <td>1381708800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0        4      True  03 11, 2014  A240ORQ2LF9LUI  0077613252   \n",
       "1        4      True  02 23, 2014  A1YCCU0YRLS0FE  0077613252   \n",
       "2        1      True  02 17, 2014  A1BJHRQDYVAY2J  0077613252   \n",
       "3        3      True  02 17, 2014   APRDVZ6QBIQXT  0077613252   \n",
       "4        5     False  10 14, 2013  A2JZTTBSLS1QXV  0077775473   \n",
       "\n",
       "                        style         reviewerName  \\\n",
       "0  {'Format:': ' Loose Leaf'}           Michelle W   \n",
       "1  {'Format:': ' Loose Leaf'}  Rosalind White Ames   \n",
       "2  {'Format:': ' Loose Leaf'}       Allan R. Baker   \n",
       "3  {'Format:': ' Loose Leaf'}                 Lucy   \n",
       "4                         NaN            Albert V.   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  The materials arrived early and were in excell...   \n",
       "1  I am really enjoying this book with the worksh...   \n",
       "2  IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...   \n",
       "3  This book was missing pages!!! Important pages...   \n",
       "4  I have used LearnSmart and can officially say ...   \n",
       "\n",
       "                         summary  unixReviewTime vote image  \n",
       "0                 Material Great      1394496000  NaN   NaN  \n",
       "1                         Health      1393113600  NaN   NaN  \n",
       "2             ARE YOU KIDING ME?      1392595200    7   NaN  \n",
       "3                missing pages!!      1392595200    3   NaN  \n",
       "4  Best study product out there!      1381708800  NaN   NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('Software.json',lines=True,)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have many features such as reviewerID that are relevant to our classifification. Hence, we filtered out all of the unverified reviews and non-relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    156117\n",
       "1     56416\n",
       "4     50506\n",
       "3     27002\n",
       "2     19304\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns = ['reviewTime','reviewerID','asin','style','reviewerName','unixReviewTime','image','vote'])\n",
    "df = df[df['verified'] == True]\n",
    "df['overall'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue that we've observed during our testing is that having an imbalanced dataset (i.e. having too many data points of one class and too few of another) significantly decreases the accuracy of our classifications. Since we also wanted to reduce the data size to reduce runtime, we balanced the dataset such that each class has an equal amount of datapoints "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, looking at our confusion matrix during testing, we notice 5 and 4 stars reviews and 1 and 2 star reviews were too similar for our classification models to distingush them. Hence, we combined our class 5 and 4 into class 5, class 1 and 2 into class 1. So, our dataset has 3 classes: 5 = postive review, 3 = mixed review, and 1 = negative review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Thought this was the product that would solve ...</td>\n",
       "      <td>Not what I was looking for.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>times out too often.</td>\n",
       "      <td>Three Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Read what is being sold in the fine print care...</td>\n",
       "      <td>Bait &amp; switch seller.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>I have used this software for years.  This yea...</td>\n",
       "      <td>Like the software but the Amazon Download vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>TurboTax did the job of doing  the taxes, but ...</td>\n",
       "      <td>TurboTax Can Be Improved</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified                                         reviewText  \\\n",
       "0        3      True  Thought this was the product that would solve ...   \n",
       "1        3      True                               times out too often.   \n",
       "2        3      True  Read what is being sold in the fine print care...   \n",
       "3        3      True  I have used this software for years.  This yea...   \n",
       "4        3      True  TurboTax did the job of doing  the taxes, but ...   \n",
       "\n",
       "                                             summary  \n",
       "0                        Not what I was looking for.  \n",
       "1                                        Three Stars  \n",
       "2                              Bait & switch seller.  \n",
       "3  Like the software but the Amazon Download vers...  \n",
       "4                           TurboTax Can Be Improved  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df[df['overall']==3].sample(6000)\n",
    "for i in range(5):\n",
    "    if i+1 != 3:\n",
    "        data = data.append(df[df['overall'] == i+1].sample(3000), ignore_index=True)\n",
    "\n",
    "data.loc[data[data['overall']==2].index, 'overall'] = 1\n",
    "data.loc[data[data['overall']==4].index, 'overall'] = 5\n",
    "data = data.dropna(subset=['reviewText'])\n",
    "data = data.dropna(subset=['summary'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main challenges of text classification is transforming the text data such that it can be trained and tested by a classification model. One simple method of transforming the data is vectorizing it into a \"Bag of Words\". A Bag of Words usually consists of two main components: a list of all of the words used in our dataset and a measure of the presence/relevance of each word relative to each data point/review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['reviewText'], data['overall'], test_size = 0.3, random_state=0, shuffle= True, stratify=data['overall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest forms of a Bag of Words is a matrix that counts the occurrance of each words in each datapoint/review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cbag.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('count',CountVectorizer()), ('clf', LogisticRegression(max_iter=3000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1160,  447,  193],\n",
       "       [ 448,  915,  437],\n",
       "       [ 152,  371, 1277]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.64      0.65      1800\n",
      "           3       0.53      0.51      0.52      1800\n",
      "           5       0.67      0.71      0.69      1800\n",
      "\n",
      "    accuracy                           0.62      5400\n",
      "   macro avg       0.62      0.62      0.62      5400\n",
      "weighted avg       0.62      0.62      0.62      5400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, simply using a counting Bag of Words and logistic regression yield a pretty good accuracy of 62%. Looking at our confusion matrix, we see that many of the misclassification are due to reviews from class 3, which is understandable since these 3 star reviews are usually a mixed bag of positive and negative sentiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how we can improve the performance of our classifications. Instead of using a bag of words that simply counts the occurances of words, we use a Bag of Words that uses a better measure of the relevance of a word in a review: Term Frequency-Inverse Document Frequency (TF-IDF). The TF-IDF of any word is calculated from the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"tfidf.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, our TF-IDF Bag of Words is just a matrix with the TF-IDF of each word in each review. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tfidfbag.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = Pipeline([('tfidf',TfidfVectorizer()), ('clf2', LogisticRegression(max_iter=3000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1252,  400,  148],\n",
       "       [ 450,  977,  373],\n",
       "       [ 141,  386, 1273]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.fit(X_train, y_train)\n",
    "y_pred = clf2.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.70      0.69      1800\n",
      "           3       0.55      0.54      0.55      1800\n",
      "           5       0.71      0.71      0.71      1800\n",
      "\n",
      "    accuracy                           0.65      5400\n",
      "   macro avg       0.65      0.65      0.65      5400\n",
      "weighted avg       0.65      0.65      0.65      5400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a TF-IDF Bag of Words improved the performance of our classification for all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue of classifying text data is that text data is messy. It is not uncommon for people to misspell or put nonsense in their reviews. Natural Language Processing (NLP) tries to understand and analyze the contents of human languages. In our project, we noticed that the number of features/words in our Bag of Words was quite large. Hence, we utilized NLP techniques such as tokenization, lemmatization, removing stop words, and normalizing cases to not only reduce the number of features in our Bag of Words but also to create a more accurate data set. For example, before using NLP, 'run' and 'running' would be considered as two different features even if their meaning is interchangable. Using lemmatization solves this issue by transforming every word into their root form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nlp.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "stopwords = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    tokens = []\n",
    "    for token in  doc:\n",
    "        if token.lemma_!= \"-PRON-\":\n",
    "            temp = token.lemma_.lower().strip()\n",
    "        else:\n",
    "            temp = token.lower_\n",
    "        tokens.append(temp)\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords and token not in punct:\n",
    "            cleaned_tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer = tokenize)\n",
    "classifier = LogisticRegression(max_iter=3000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['summary'], data['overall'], test_size = 0.3, random_state=0, shuffle= True, stratify=data['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('tfidf',tfidf), ('clf', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function clean_data at 0x000001FA61B7F0D8>)),\n",
       "                ('clf', LogisticRegression(max_iter=3000))])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.69      0.68      1800\n",
      "           3       0.60      0.59      0.59      1800\n",
      "           5       0.74      0.73      0.73      1800\n",
      "\n",
      "    accuracy                           0.67      5400\n",
      "   macro avg       0.67      0.67      0.67      5400\n",
      "weighted avg       0.67      0.67      0.67      5400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1247,  411,  142],\n",
       "       [ 417, 1062,  321],\n",
       "       [ 184,  305, 1311]], dtype=int64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing our text data by using lemmatization, normalizing cases, and removing stop words and punctuations improved the performance of our classification. Most notably, the accuracy of our class 3 classifications improved significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this project, we used logistic regression, but we also tested many other classification models. For example, we tested: Random Forest, Bayes Nets, and Support Vector Machine. But ultimately, the logisitic regression actually performed the best. For example, it obtained an 1-2% higher accuracy than SVC throughout several runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.68      0.68      1800\n",
      "           3       0.60      0.57      0.59      1800\n",
      "           5       0.71      0.73      0.72      1800\n",
      "\n",
      "    accuracy                           0.66      5400\n",
      "   macro avg       0.66      0.66      0.66      5400\n",
      "weighted avg       0.66      0.66      0.66      5400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_classifier = LinearSVC()\n",
    "clf = Pipeline([('tfidf',tfidf), ('clf', svc_classifier)])\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classier that we've made performs decently well, however, there is still a lot of room for improvement. For example, there are still many NLP techiqnes such as POS tagging and Dependency parsing that was not used in our algorithm. Additionally, it also possible that we can improve our classification model by imploding a deep learning and building a neutral network. Furthermore, instead of a vectorizing our data into a Bag of Words, we can try to see if vectorizing it into an N-Gram can improve its perfomance. Although our algorithm was made particularly for sentiment. the NLP techniques and the classification used in this project have many other applications such as chat bots, speech recognition, and spam filters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
